# ReadingList
This is the reading list of topics 

###### CVPR 2019 about interpretable model ############   update Feb/02/2919  ###########
1. [Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Exploiting_Kernel_Sparsity_and_Entropy_for_Interpretable_CNN_Compression_CVPR_2019_paper.pdf)

2. [Interpreting CNNs via Decision Trees](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.pdf)

3. [End-To-End Interpretable Neural Motion Planner](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zeng_End-To-End_Interpretable_Neural_Motion_Planner_CVPR_2019_paper.pdf)

4. [Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)


#### [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)  ICLR2017  Nov/20/2019
1. Goal: Using RNN to design CNN and RNN architecture with Reinforcement Learning
2. Reward is accuracy on test dataset. 
3. using Signmod to choose settings from sets.
4. Heigh lights 1. variable length and structure. get some novel structures. 2. design structure automatically. 
                  



### [Natrual language understanding]:
   NLU = NLP + IR ? 
### [Named Entity Recognition](https://github.com/JiazhaoLi/NLP_ReadingList/blob/master/NER.md):
   
### [Question Answering](https://github.com/JiazhaoLi/NLP-Reading-List/blob/master/QA.md)
   
      

## Latest NLP models: 
   update:Sep.2.2019 
   
   [ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1907.12412.pdf)
   
   [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
   
   [BERT](https://arxiv.org/abs/1810.04805)
   
   [ELMo](https://arxiv.org/abs/1802.05365)
   
## word embedding:
   1.word2vec
   2.GloVe
   3.Fasttext
   
## subword embedding:
   1. Byte Pair Encoding (BPE)
   2. WordPiece
   3. Unigram Language Model
 
## GLUE Tasks:
   1.[The Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/)
## Text Generation:
  - Machine Translation
  - 

## Text Classification:
 - single-sentence classification
 - pairwise text classification
 - pairwise text similarity 
 - relevance ranking


## [SuperGLUE Task](https://super.gluebenchmark.com/)ï¼š
  #[ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1907.12412.pdf)

