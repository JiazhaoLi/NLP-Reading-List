# NLP_ReadingList
This is the reading list of topics in NLP area



#### [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)  ICLR2017  Nov/20/2019
1. Goal: Using RNN to design CNN and RNN architecture with Reinforcement Learning
2. Reward is accuracy on test dataset. 
3. using Signmod to choose settings from sets.
4. Heigh lights 1. variable length and structure. get some novel structures. 2. design structure automatically. 
                  



### [Natrual language understanding]:
   NLU = NLP + IR ? 
### [Named Entity Recognition](https://github.com/JiazhaoLi/NLP_ReadingList/blob/master/NER.md):
   
### [Question Answering](https://github.com/JiazhaoLi/NLP-Reading-List/blob/master/QA.md)
   
      

## Latest NLP models: 
   update:Sep.2.2019 
   
   [ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1907.12412.pdf)
   
   [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
   
   [BERT](https://arxiv.org/abs/1810.04805)
   
   [ELMo](https://arxiv.org/abs/1802.05365)
   
## word embedding:
   1.word2vec
   2.GloVe
   3.Fasttext
   
## subword embedding:
   1. Byte Pair Encoding (BPE)
   2. WordPiece
   3. Unigram Language Model
 
## GLUE Tasks:
   1.[The Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/)
## Text Generation:
  - Machine Translation
  - 

## Text Classification:
 - single-sentence classification
 - pairwise text classification
 - pairwise text similarity 
 - relevance ranking


## [SuperGLUE Task](https://super.gluebenchmark.com/)ï¼š
  #[ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1907.12412.pdf)

